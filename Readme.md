## ❤️项目详解❤️
爬虫 和 Python零基础入门项目, 带详非常细中文注释.
建议按照顺序 先了解简单的.

#### 1. mezitu-done.py   ✔    
最最基础的爬虫,Mac OS 2017-03-27-10 亲测可用.
可以爬整个 www.mzitu.com里面的所有写真照片.
在当前脚本建立一个 妹子图总文件夹.
里面按照各个主题名建立子文件夹.并下载对应的照片.


#### 2. meizitu-pause.py    暂停ing....    
可以爬整个 www.meizitu.com里面的所有写真照片.
你看懂了 第一个爬虫. 就可以自己学这个爬虫了.
代码非常非常类似!!!
突然发现 爬图片返回的是403状态码..
有空再折腾... 就差一步下载了.








#### 10. 知乎.py   ✗   
自动登录知乎用的... 有问题!! 还在调试中.....









## ❤️项目基本信息❤️


爬虫的学习路线是陡峭的。  
入门很容易，随手google一下10行代码就可以抓取一个静态网页的代码。  
想学习爬虫，下面这些东西跳不过去，可以以后慢慢掌握,但是必须先有点了解.

- 任意一门编程语言
- 网页前端基本知识
- 数据库基本使用
- 计算机网络协议
- 前端加密算法（如果你做产品抓商业化的信息，那这是不可少的）
- 了解抓包，模拟发送请求等等
- 其他验证码，ip地址等等

 


## ❤️爬虫背景 & 简介❤️

IT作为今天最火热的行业之一，其中又衍生机器学习，深度学习，数据挖掘，等等。  
这些学科能够发展，离不开数据总量快速的增长，以及现在电脑对信息的快速处理。  
显然，爬虫已经逐渐成为了今天的程序员或者计算机爱好者的被动技能。 

作为个人学习，没有必要去做一个完善的自动化爬虫，
只需能够抓取我们想要的信息就可以了。 

#### 网络爬虫:
英语：web crawler 也叫网络蜘蛛 spider  
一种根据一定规则来浏览万维网的网络机器人。   
可以自动获取网互联网信息的程序或脚本,提取网页中有价值的数据.  
    - 可以爬某个网站下的某些数据.  
    - 也可以爬全世界所有网站下的某些数据.   
其目的一般为编纂网络索引,也是搜索引擎最基本的原理。  
据说google 最初就是用python写了网页爬虫. 

❗️❗️爬虫: 搜索引擎的基础!❗️❗️   
你写了一篇文章.发表到博客上. 谷歌凭什么能搜索到你的文章!   
谷歌有很多高级的爬虫. 这些爬虫无时无刻都在互联网上抓取信息.  
只有某个谷歌爬虫爬到了你发表的文章. 你的文章才会被收录  
只有被收录后,你的网页才会出现在谷歌搜索结果中!!!    


#### 互联网: 
无数网页组成.  
网页使用 URL  网址互相访问.   
网页使用 HTTP 协议传输数据给服务器.  
网页使用 HTML 语言编写成的.

互联网就像一张超级巨大的蜘蛛网. 爬虫就是里面的一中小蜘蛛.  
每个网页都有很多链接(URL).理论上通过这些链接可以通到世界上任意一个网站.  
爬虫日夜不停的去查看所有网页.遇到有价值的东西就下载下来.  




## ❤️爬虫用途:❤️

- 抓取知乎数据分析知乎用户并做出图谱。
- 抓取网页云音乐评论做自然语言处理得到人们对不同歌手的评价。
- 抓取豆瓣电影按评分挑出自己喜欢看
- 周边租房的房价
- 网站各种优惠?
- 知乎上点赞数很高的文章..
- 快手粉丝数量..
- 某站的所有图片,电影!!
- 价值最高的还是用来做商业分析，这也催生了爬虫工程师这个职业。
- 研究SEO. 就得知道爬虫.
- 收集大量的代理服务器!!来获得不同的IP 让后刷票什么的!!!!
- 抓twitter 上亿条的的说说..储存到数据库.更深入的了解数据库
- 获取某网站所有用户的头像.更具头像被点击的次数.找出最受欢迎的头像
- 抓取 facebook 上的 sleep关键词. 来判断出大家的睡眠时间..很多人喜欢在睡前会说一声我睡了。
- 喜欢看美女图片?  写个爬虫把某网站所有的图片都下下来 慢慢看.
- 喜欢看电影?      写个爬虫把某网站所有种子都下下来,慢慢下载.
- 喜欢某个妹子?    写个爬虫把她所有说说都下下来研究
- 喜欢秒杀超值物品 写个爬虫.... 还是脚本 ??
- 比价网站网站 靠爬虫实现的.
- 电影推荐网站 靠爬虫实现的.



## ❤️爬虫原理:❤️

1. 抓取网页源代码: 源代码中的图片,视频都是链接! 源代码是全文本的.不大
2. 分析网页源代码: 你要下载图片/视频/网址.就分析出源代码里面的图片/视频/网址的链接. 
3. 储存信息:       分析出来后 就要储存起来了.文本/视频/图片....
5. 抓取下一个网页. 循环就能获取到很多很多信息了.


抓取网页:  
    平时上网,浏览器中输入网址,按下回车. 浏览器会显示出网页内容.  
    上网过程原理: 发送请求 + 接收请求 + 渲染 
      发送请求里面 有要访问的网址  
      接收请求里面 有要显示的内容
      渲染大概就是把图片链接显示成图片. 把视频链接显示成视频  
    浏览器都可以上网.更别说用终端了.  


## 爬虫储备知识:

HTML 下载器: 
    urllib2 , requests
    下载网页,保存成字符串.还可以 提交请求

URL 解析器:  
    管理要抓取的URL  和 已经抓取过的URL.
    beautifulsoup
    一方面抓取有价值的数据. 一方面把新的URL补充到 URL管理器

经典爬虫框架: Scrapy


#### User-Agent

 一般网站其实是不喜欢爬虫的.
 网址就是靠信息的.你爬虫大量抓取某网站的信息.
 网站一般会对 请求头 request header 做一些限制，
 不是浏览器 不回复你, 就是为了反爬虫...
 就需要设置一个 User-Agent 来骗对方的服务器

 http 请求状态有很多种，大多数情况下 200 是我们理想中的状态，
 我们用 requests 的一个 status_code 方法来判断是否访问成功，

 由于网络原因，可能会出现访问超时的问题，
 为了让代码更健壮，我们可以设置 timeout, 
 同时，如果确定网页不需要重定向， 可以设置 allow_redirects=False









## 🎈爬虫相关模块: 🎈

### 📌 下载模块: urllib2 == urllib.request 📌
    这模块是下载网页的!!!
    爬虫么肯定要先下载网页 再分析网页了. 
    
    import urllib2         是python2 的语法.
    import urllib.request  是python3 的语法

遇到报错:
要么是你用python3 执行一个脚本. 这里脚本里面用了import urllib2 
要么是你用python2 执行一个脚本. 这里脚本里面用了import urllib.request 
这里只说了引入模块的区别. 具体的用法也是有区别的.


### 📌 解析模块: Beautiful Soup 📌
    分析网页最流行的模块.提取网页中特定数据用的.

    中文文档:
    https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html
    
    安装这个模块后必须安装一个解析器. 一般用 lxml
    pip3 install lxml


不管爬什么. 解析网页结构是最基础的.
推荐用 xpath. 可以从网页中 提取出某元素.
重点是 谷歌开发工具 → 选择某个元素 → 右击 → 复制 → 可以选择. xpath.
都不用你学!!! 直接帮你写好路径










### 📌 路径模块: os 📌
    系统路径相关,也是常用的.
    查看当前运行脚本的路径.
    新建/删除文件夹.
    判断文件夹是否存在



### 📌 正则式模块: re 📌

常用的是  
贪婪匹配 和 非贪婪匹配.
单行匹配 和 全文匹配.










## ❤️❤️ 必学技能 ❤️❤️
最基础的就是网页分析.
从一个网址中 提取出某个数据.
这需要不断的调试 才能取得我们要的值.
去终端里调试!!!!

这里用都 scrapy 这个模块.
先用这个模块建立一个项目.
再cd到那个项目. 用下面命令 就可以获取实时的结果了..
scrapy shell "https://www.0214.live"
'' // 需要到项目里去运行..
'' ★取出网站标题★
'' >>> response.css('title::text').extract()
'' [u'Xu.Jian']

这里出来 乱码问题. 英文正常中文..... 不正常..


















## 🎈Python 简介:🎈
 	Python的灵魂在于各种优秀的库/框架/模块.
    爬虫一般都是用python写的!!! 所以需要了解下python
    Python非常简单. 如果你有别的编程语言基础.几乎可以不用学习.
    就能大概看懂别人的python脚本的内容.
    当然下面列出来的你必须知道.基础的基础.


## 🎈Python 版本:🎈
    2.x  和3.x  不兼容的!!!!!
    你网上看的教程.必须先知道对方用的是哪个版本.
    Mac 升级Python 3.x:  brew install python3

#### Python 版本切换.
📌📌要用python3 执行命令时候 python3 xxx.py 📌📌  
📌📌要用python2 执行命令时候 python2 xxx.py 📌📌

用type+命令 来查看命令的启动路径.  
✘✘∙𝒗 zz type python  
python is /usr/local/bin/python  
✘✘∙𝒗 zz type python3  
python3 is /usr/local/bin/python3  

📌Mac 系统自带的python环境 默认启动路径是 /usr/bin         📌  
📌Mac 用户安装的python环境 默认启动路径是 /usr/local/bin   📌  
📌终端里 用python xxx.py 默认使用/usr/local/bin 下的python 📌  



## 🎈Python IDE: VS code 🎈
    只有两个. Pycharm 和 VScode
    Pycharm: 专业的 Python 编辑器. 内存占用大,上手难!初学者不建议用.
    推荐 VScode... 

    📌📌VS code Python 环境:📌📌
        VS code 原生是不支持Python的装个 python 插件就能用了.
        这个插件自带 调试功能
        具体配置看这篇文章
        http://www.jianshu.com/p/c76d616a23ff



## 🎈Python 基本用法:🎈

#### 注释:   
单行注释: 井号 #  右边的任何内容都会被忽略.  
多行注释: 三个单引号. 或者三个双引号.  


#### 📌模块安装:📌  
一般都用 pip 来安装各种模块.  
    pip install xxx 是在python2版本下面安装模块.  
pip3 install xxx 是在python3版本下面安装模块.  
记住,模块是不通用的.   
反正出现找不到模块这种错误你就重新安装就可以了.  




## 🎈Python 常见错误:🎈

    ❌ shell 下方向键乱码
        按方向键就出来 ^[[A^[[B^[[D^[[C 
        这些其实是方向键盘的 acsll 码.
        安装下 readline  就好了!!!!!!!
        easy_install readline


    ❌ 各种缩进错误
        📌📌📌python 是根据缩进判断嵌套关系的!      📌📌📌
        📌📌📌多用tab, 少空格!                      📌📌📌
        📌📌📌不要打多余的空格, 哪怕是注释行的尾部  📌📌📌
        📌📌📌pylint 最好的Python代码分析工具.
              VScode装了python插件就自动开启

        C0325: Unnecessary parens after u'print' keyword，
            print("Hello") 出现的绿波浪线，其实print "hello"就足够了.

        C0303: Trailing whitespace      行尾不能有空格!!!!   trail 尾巴.
        C0304: Final newline missing，  最后一行下要留出一个空行。
        E116:  unexpected indentation   不正常缩进!!



























