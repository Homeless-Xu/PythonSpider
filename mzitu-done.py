#!/usr/bin/python3
# -*- coding: utf-8 -*-

# æœ¬è„šæœ¬ç”¨çš„æ˜¯ Python3; 
# ç”¨python3 è¿è¡Œpyè„šæœ¬ æœ€ç®€å•çš„åŠæ³•æ˜¯ python3 xxx.py
# æœ¬æ–‡ #   å·æ³¨é‡Šçš„æ˜¯æ–‡å­—è¯´æ˜.
# æœ¬æ–‡ ''' å·æ³¨é‡Šçš„æ˜¯ä»£ç . å„ç§è°ƒè¯•ä»£ç . æ–¹ä¾¿ä½ è°ƒè¯•.


'''
â¤ï¸â¤ï¸ èƒŒæ™¯ç®€ä»‹ â¤ï¸â¤ï¸

 ğŸˆå¦¹å­å›¾:ğŸˆ
    æœ‰ä¸¤ä¸ª:  å›¾ç‰‡éƒ½ä¸€æ ·.
    www.meizitu.com æ²¡å¹¿å‘Š, ä¸€é¡µå¤šå¼ ç…§ç‰‡. ä¸å¥½çˆ¬
    www.mzitu.com   å¹¿å‘Šå¤š, ä¸€é¡µä¸€å¼ ç…§ç‰‡. æœ‰æ°´å°,å®¹æ˜“çˆ¬
    å…ˆæ¥ç®€å•çš„ï¼Œäºæ˜¯çˆ¬äº†www.mzitu.com

 ğŸˆç½‘ç«™åˆ†æğŸˆ:
     å¦¹å­å›¾å‡ ä¹æ¯å¤©éƒ½æ›´æ–°ï¼Œ
     åˆ°ç°åœ¨ä¸ºæ­¢æœ‰ 140é¡µ
     æ¯é¡µ24ä¸ªä¸»é¢˜å†™çœŸï¼Œ
     æ¯ä¸ªä¸»é¢˜ä¸‹æœ‰å‡ åå¼ ç…§ç‰‡ï¼Œæ¯å¼ ç…§ç‰‡ä¸€ä¸ªç½‘é¡µ.
     ç½‘é¡µç»“æ„ç®€å•.ç”¨ BeautifulSoup å°±å¯ä»¥è½»æ¾çˆ¬å–ã€‚

    ğŸ“Œ ç½‘ç«™140+é¡µ.  æ¯é¡µçš„ç½‘å€å¾ˆæœ‰è§„å¾‹ 1-140
        åªè¦èƒ½è·å¾—ä¸€ä¸ªé¡µé¢é‡Œé¢çš„æ•°æ®
        å‰©ä¸‹é¡µé¢çš„æ•°æ®åªè¦ä»1åˆ°140 å¾ªç¯.å°±å¯ä»¥äº†
        http://www.mzitu.com/page/1
        http://www.mzitu.com/page/2
        http://www.mzitu.com/page/3
        ......
        http://www.mzitu.com/page/140

    ğŸ“Œ æ¯é¡µ24ä¸ªä¸»é¢˜. æ¯ä¸ªä¸»é¢˜ä¸€ä¸ªé“¾æ¥.
        http://www.mzitu.com/87933
        http://www.mzitu.com/87825
        æ¯ä¸ªä¸»é¢˜ä¹‹é—´å°±æ²¡ä»€ä¹ˆè”ç³»äº†.
        æ‰€æœ‰ä¸»é¢˜çš„ç½‘å€å°±å¾—æ‰‹åŠ¨çˆ¬ä¸‹æ¥.
        è¿™é‡Œå°±ä¸èƒ½ç”¨å¾ªç¯äº†...

    ğŸ“Œ æ¯ä¸ªä¸»é¢˜è¯ºå¹²å¼ å›¾ç‰‡. æ¯å¼ å›¾ç‰‡ä¸€ä¸ªç½‘å€
        http://www.mzitu.com/86819/1
        http://www.mzitu.com/86819/2
        http://www.mzitu.com/86819/3
        å•ä¸ªä¸»é¢˜ä¸‹çš„å›¾ç‰‡å¾ˆæœ‰è§„å¾‹
        åªè¦çŸ¥é“è¿™ä¸ªä¸»é¢˜çš„å›¾ç‰‡æ•°é‡å°±èƒ½å¾ªç¯å‡ºæŸä¸»é¢˜ä¸‹æ‰€æœ‰çš„ç½‘å€.
        è¿™ä¸ªç½‘å€ ä¸ç­‰äº å›¾ç‰‡çš„ç½‘å€.
        å›¾ç‰‡ç½‘å€ éœ€è¦åˆ°æ¯ä¸ªç½‘é¡µä¸‹é¢åŒ¹é…å‡ºæ¥.



 ğŸˆçˆ¬è™«æ­¥éª¤ï¼šğŸˆ
     æ•´ä¸ªå¦¹å­å›¾æ‰€æœ‰ä¸»é¢˜çš„ç½‘å€.            get_page1_urls
     æŸä¸»é¢˜ä¸‹ç¬¬ä¸€å¼ ç…§ç‰‡åœ°å€               get_img_url
     æŸä¸»é¢˜çš„ç…§ç‰‡æ•°                       get_page_num
     ç”¨å¾ªç¯è·å–æŸä¸»é¢˜ä¸‹æ‰€æœ‰ç…§ç‰‡åœ°å€       get_img_url
     è·å–å„ä¸ªä¸»é¢˜çš„ä¸»é¢˜åå­—               get_img_title
     ä¸‹è½½æ‰€æœ‰ä¸»é¢˜ä¸‹çš„æ‰€æœ‰ç…§ç‰‡             download_imgs

'''


# â¤ï¸â¤ï¸ â†“â†“â†“ 0: ä¾èµ–æ¨¡å— â†“â†“â†“ â¤ï¸â¤ï¸


import urllib.request          # è·å–ç½‘é¡µå†…å®¹
from bs4 import BeautifulSoup  # è§£æç½‘é¡µå†…å®¹
import re                      # æ­£åˆ™å¼æ¨¡å—.
import os                      # ç³»ç»Ÿè·¯å¾„æ¨¡å—: åˆ›å»ºæ–‡ä»¶å¤¹ç”¨
import socket                  # ä¸‹è½½ç”¨åˆ°?
import time                    # ä¸‹è½½ç”¨åˆ°?


# â¤ï¸â¤ï¸ â†“â†“â†“ 0åŸºç¡€å…¥é—¨ä¾‹å­ â†“â†“â†“ â¤ï¸â¤ï¸


'''
resp = urllib.request.urlopen('http://www.mzitu.com/page/1')
html = resp.read()
print(html)
'''
# æµè§ˆå™¨è¾“å…¥ç½‘å€å°±ä¸‹è½½ç½‘é¡µå¹¶æ˜¾ç¤ºç»™ä½ .
# ç»ˆç«¯æ€ä¹ˆä¸‹è½½ç½‘é¡µå‘¢.  ç”¨ urllib2 / urllib.request æ˜¯æœ€æ–¹ä¾¿çš„.
# é¦–å…ˆpython3 çš„è¯:  import urllib.request å¯¼å…¥è¿™ä¸ªä¸‹è½½ç½‘é¡µçš„åº“.
# ç„¶åä¸Šé¢ç¬¬ä¸€è¡Œåªè¦æ”¹æ‰urlopen åŒå¼•å·é‡Œé¢çš„ç½‘å€å°±å¯ä»¥çˆ¬ä½ æƒ³è¦çš„ç½‘å€äº†.
# å°±å¯ä»¥åœ¨ç»ˆç«¯æ˜¾ç¤ºå‡º æŸä¸ªç½‘ç«™çš„æºä»£ç äº†, è¿™æ ·ä¸€ä¸ªç½‘é¡µå°±ç®—è¢«ä½ çˆ¬ä¸‹æ¥äº†!! å¾ˆç®€å•å§.
# response = urllib2.urlopen("https://www.0214.live")
# è°ƒç”¨ urllibåº“ é‡Œé¢çš„ urlopen è¿™ä¸ªæ–¹æ³•,è¿™ä¸ªæ–¹æ³•éœ€è¦ä¼ å…¥ä¸€ä¸ªæ•°æ®.ä¸€èˆ¬éƒ½æ˜¯ç½‘å€.
# æŠŠ æ•´ä¸ªç½‘é¡µæ•°æ®ä¿å­˜åˆ° response è¿™ä¸ªå˜é‡ä¸­.
# è¿™è¡Œå…¶å®å¯ä»¥åˆ†æˆä¸¤è¡Œ.
# request = urllib2.Request("https://www.0214.live")
# response = urllib2.urlopen(request)
# æ¨èå¤§å®¶è¿™ä¹ˆå†™. æœ€ç»ˆä¼šæœ‰å¾ˆå¤šä»£ç . è¿™æ ·å†™é€»è¾‘ä¸Šæ›´æ¸…æ™°.
# å…ˆä¼ å…¥ä¸€ä¸ªè¯·æ±‚,  ç„¶åå†æ‰“å¼€ç½‘ç«™
# print response.read()
# è¯»å–å˜é‡çš„å†…å®¹. ä¹Ÿå°±æ˜¯æ•´ä¸ªç½‘é¡µçš„å†…å®¹

# ä¸Šé¢ä¸‰è¡Œæ˜¯è·å–ç½‘é¡µæºä»£ç çš„æ–¹æ³•.
# å–æ¶ˆæ³¨é‡Š. å°±èƒ½åœ¨ç»ˆç«¯æ˜¾ç¤ºå¦¹å­å›¾ç¬¬ä¸€é¡µçš„æºä»£ç äº†.
# æ¥ä¸‹æ¥éœ€è¦ä»æºä»£ç é‡Œé¢. æ‰¾å‡ºç¬¬ä¸€é¡µçš„24ä¸ª ä¸»é¢˜åœ°å€ç½‘å€!
# è¿™é‡Œå°±è¦ç”¨ beautifulsoup 4 æ¨¡å—æ¥æå–ç½‘é¡µä¸­æŒ‡å®šå†…å®¹äº†. ç®€ç§° bs4
# ä¸‹é¢æ˜¯ bs4 çš„åŸºæœ¬ç”¨æ³•.
'''
# soup = BeautifulSoup(html, "lxml")
# æŠŠç½‘é¡µæºä»£ç  ç»“æ„åŒ–æˆ DOMæ ‘.
# ä¹‹åå°±å¯ä»¥ç”¨ soup.title; soup.p; soup.a; æ¥è¾“å‡ºç½‘é¡µé‡Œé¢çš„å†…å®¹äº†.
# ä½ åªè¦æ”¹ html è¿™ä¸ªå˜é‡å°±å¯ä»¥äº†. åé¢çš„è§£æå™¨ä¸è¦åŠ¨.

# print(soup.title)
# â†’ <title>å¦¹å­å›¾_æ€§æ„Ÿå¦¹å­å›¾ç‰‡_æ—¥æœ¬ç¾å¥³å›¾ç‰‡_æ¸…çº¯å¦¹å­å†™çœŸ </title>
# è¾“å‡ºç½‘é¡µæ ‡é¢˜(å¸¦æ ‡ç­¾çš„!)

# print(soup.title.string)
# â†’ å¦¹å­å›¾_æ€§æ„Ÿå¦¹å­å›¾ç‰‡_æ—¥æœ¬ç¾å¥³å›¾ç‰‡_æ¸…çº¯å¦¹å­å†™çœŸ 
# è¾“å‡ºç½‘é¡µæ ‡é¢˜(ä¸å¸¦æ ‡ç­¾çš„!,åªè¾“å‡ºæ ‡é¢˜å†…å®¹)

# print("è¿™ä¸ªç½‘é¡µæ ‡é¢˜:" + soup.title.string)
# å¦‚æœè¦è¾“å‡ºè‡ªå®šä¹‰å†…å®¹ éœ€è¦ç”¨åŒå¼•å· åŠ ä¸Š+å·

# print(soup.a)
# â†’ <a href="http://www.mzitu.com/">å¦¹å­å›¾</a>
# è¿™ä¸ªæ˜¯è¾“å‡ºç½‘é¡µä¸­é‡åˆ°çš„ç¬¬ä¸€ä¸ªé“¾æ¥.

# find_all() æœç´¢å…ƒç´ ,
# find_all('a')  â†’ æœç´¢æ‰€æœ‰çš„a  é“¾æ¥ 
# find_all('p')  â†’ æœç´¢æ‰€æœ‰çš„p  æ®µè½
# find_all('li') â†’ æœç´¢æ‰€æœ‰çš„li åˆ—è¡¨é¡¹ç›®
# find_all() ä¹Ÿèƒ½ç”¨æ­£åˆ™å¼æ¥åŒ¹é…æœç´¢,è¿™é‡Œå°±ä¸ä»‹ç»äº†..

# for link in soup.find_all('a'):
# print(link.get('href'))
# è¿™æ˜¯æ‰¾å‡ºç½‘é¡µå†…æ‰€æœ‰çš„é“¾æ¥.
# linkæ˜¯ä¸ªå˜é‡. åå­—ä»»å–. åªè¦ä¸Šä¸‹ä¸¤è¡Œå¯¹åº”å°±å¯.
# for xxx in soup.find_all('a'):
#     print(xxx.get('href'))

# æœç´¢ç½‘é¡µä¸­æ‰€æœ‰çš„ id æ˜¯pins çš„ulå…ƒç´ 
# for link in soup.find_all('ul', id="pins"):
#     print(link)
# æœç´¢ç½‘é¡µä¸­æ‰€æœ‰çš„ class æ˜¯ çš„imgå…ƒç´ 
# for link2 in soup.find_all('img', "lazy"):
#     print(link2)
# ID å’Œ Class ç¨å¾®æœ‰ç‚¹åŒºåˆ«. å¤šå‚æ•°æƒ…å†µä¸‹é»˜è®¤æ˜¯æœç´¢classçš„
'''
# äº†è§£äº†bs4 çš„åŸºæœ¬ç”¨æ³•. å°±å¯ä»¥ æŠŠç½‘å€ä¸­çš„ç‰¹å®šçš„ç½‘å€åŠ åˆ°æŸä¸ªæ•°ç»„é‡Œäº†.
# åŠŸèƒ½: æŠŠ id æ˜¯ pins çš„ulå…ƒç´  ä¸‹é¢çš„æ‰€æœ‰lié‡Œçš„é“¾æ¥ åŠ åˆ°ä¸€ä¸ªæ•°ç»„é‡Œé¢
# æ¯é¡µçš„24ä¸ªå¦¹å­ä¸»é¢˜. éƒ½æ˜¯åœ¨ <ul id="pins"> ä¸‹é¢çš„24ä¸ªliä¸­.
# lié‡ŒåŒ…å«å¾ˆå¤šæ— ç”¨çš„ä¿¡æ¯:æ¯”å¦‚æ ‡é¢˜,æ—¶é—´. æˆ‘ä»¬åªè¦é‡Œé¢çš„ç½‘å€.
# ç”¨å¾ªç¯æŠŠæ¯ä¸ªä¸»é¢˜çš„åœ°å€. æ·»åŠ åˆ°page1_urls è¿™ä¸ªæ•°ç»„é‡Œé¢.
'''
page1_urls = []
lis = soup.find('ul', {'id': 'pins'}).find_all('li')
print(lis)
for li in lis:
    page1_urls.append(li.find('a')['href'])
print(page1_urls)

# ä¸‹é¢æ˜¯ç»“æœ
# âœ˜âœ˜âˆ™ğ’— zz python3 mzitu00.py
# ['http://www.mzitu.com/88557', 'http://www.mzitu.com/88509', 'http://www.mzitu
# .com/88458', 'http://www.mzitu.com/88415', 'http://www.mzitu.com/88373', 'http
# ://www.mzitu.com/88317', 'http://www.mzitu.com/88250', 'http://www.mzitu.com/8
# 8209', 'http://www.mzitu.com/88162', 'http://www.mzitu.com/88116', 'http://www
# .mzitu.com/88065', 'http://www.mzitu.com/88014', 'http://www.mzitu.com/87933',
#  'http://www.mzitu.com/87973', 'http://www.mzitu.com/87825', 'http://www.mzitu
# .com/87813', 'http://www.mzitu.com/87762', 'http://www.mzitu.com/87693', 'http
# ://www.mzitu.com/87628', 'http://www.mzitu.com/87580', 'http://www.mzitu.com/8
# 7541', 'http://www.mzitu.com/87500', 'http://www.mzitu.com/87434', 'http://www
# .mzitu.com/87390']

# çœ‹äº†ä¸Šé¢çš„ä¾‹å­å°±ä¼šçˆ¬ç¬¬ä¸€ä¸ªé¡µé¢ä¸Šçš„24ä¸ªä¸»é¢˜çš„é“¾æ¥äº†,åªè¦ç”¨å¾ªç¯å°±èƒ½çˆ¬æ‰€æœ‰é¡µé¢äº†!!!
'''


# â¤ï¸â¤ï¸ â†“â†“â†“ è·å–æ•´ä¸ªå¦¹å­ç½‘æ‰€æœ‰çš„ä¸»é¢˜ â†“â†“â†“ â¤ï¸â¤ï¸


def get_page1_urls():          # å®šä¹‰ä¸€ä¸ªå‡½æ•°
    page1_urls = []            # å®šä¹‰ä¸€ä¸ªæ•°ç»„,æ¥å‚¨å­˜æ‰€æœ‰ä¸»é¢˜çš„URL
    for page in range(1, 140):
        # 1-140. æ•´ä¸ªå¦¹å­å›¾åªæœ‰140é¡µ,æ³¨æ„ä¸‹é¢ç¼©è¿›å†…å®¹éƒ½åœ¨å¾ªç¯å†…çš„!
        url = 'http://www.mzitu.com/page/' + str(page)
        request = urllib.request.Request(url)
        # åˆ¶ä½œè¯·æ±‚å¤´äº†. 140é¡µ æ¯é¡µéƒ½è¯·æ±‚ä¸€é. è‡ªç„¶å°±èƒ½è·å–åˆ°æ¯é¡µä¸‹çš„24ä¸ªä¸»é¢˜äº†
        html = urllib.request.urlopen(request, timeout=20).read()
        # read å°±æ˜¯è¯»å–ç½‘é¡µå†…å®¹å¹¶å‚¨å­˜åˆ° htmlå˜é‡ä¸­.
        soup = BeautifulSoup(html, 'lxml')
        # æŠŠä¸‹è½½çš„ç½‘é¡µ.ç»“æ„åŒ–æˆDOM, æ–¹ä¾¿ä¸‹é¢ç”¨ find å–å‡ºæ•°æ®
        lis = soup.find('ul', {'id': 'pins'}).find_all('li')
        # æ‰¾åˆ° id ä¸ºpins è¿™ä¸ªåˆ—è¡¨ä¸‹é¢çš„ æ¯ä¸ªåˆ— å°±æ‰¾åˆ°æ¯ä¸ªé¡µé¢ä¸‹çš„ 24ä¸ªä¸»é¢˜äº†
        for li in lis:
            # éå†æ¯é¡µä¸‹é¢çš„24ä¸ªä¸»é¢˜ (ä¹Ÿå°±æ˜¯24ä¸ªli)
            page1_urls.append(li.find('a')['href'])
            # æŠŠæ¯ä¸ªä¸»é¢˜çš„åœ°å€. æ·»åŠ åˆ°page1_urls è¿™ä¸ªæ•°ç»„é‡Œé¢.
        # print(page1_urls)
        # # æ˜¾ç¤ºç½‘å€. æµ‹è¯•ç”¨. å¾ªç¯140æ¬¡. è¿™æ ·å°±è·å¾—äº†æ‰€æœ‰ä¸»é¢˜çš„ç½‘å€äº†
    return page1_urls


# â¤ï¸â¤ï¸ â†“â†“â†“ è‡ªåŠ¨è·å–æŸä¸»é¢˜çš„ç…§ç‰‡æ•°é‡ â†“â†“â†“ â¤ï¸â¤ï¸
# è¿›å…¥æŸä¸ªä¸»é¢˜, ç„¶ååˆ†æåº•éƒ¨çš„ å¯¼èˆªæ¡.
# å¯¼èˆªæ¡æ ¼å¼: ä¸Šä¸€ç»„ 1 2 3 4 ... 64 ä¸‹ä¸€ç»„ 
# å¾ˆå¤šæŒ‰é’®.æ¯ä¸ªæŒ‰é’®éƒ½æ˜¯ä¸€ä¸ª<a>å…ƒç´ . 
# å€’æ•°ç¬¬äºŒä¸ª<a>å…ƒç´  è¿™é‡Œä¹Ÿå°±æ˜¯64 å°±æ˜¯ç…§ç‰‡æ•°é‡!


def get_page_num(page1_url):        # å‚æ•° page1_url ä¸ä¸€å®šè¦å¤–ç•Œä¼ å…¥çš„. å¯ä»¥ç»™å‡½æ•°é‡Œé¢ç”¨çš„.
    request = urllib.request.Request(page1_url)
    try:
        html = urllib.request.urlopen(request, timeout=20).read()
    except:
        try:
            html = urllib.request.urlopen(request, timeout=20).read()
        except:
            return None
            # è¿™ä¸ªå‡½æ•°ä¼šé‡å¤è¯·æ±‚ä¸¤æ¬¡. å¦‚æœä¸¤æ¬¡éƒ½è¶…æ—¶å°±æ”¾å¼ƒ.
    soup = BeautifulSoup(html, 'lxml')
    try:
        page_num = soup.find('div', {'class': 'pagenavi'}).find_all('a')[-2].find('span').get_text()
    except:
        return None
    return int(page_num)
# aa = get_page_num("http://www.mzitu.com/858")
# print(aa)
# è¿™ä¸¤è¡Œæ˜¯æµ‹è¯• æŸä¸»é¢˜ä¸‹çš„å›¾ç‰‡æ•°é‡çš„. ä½ éšä¾¿å¡«ä¸ªå¦¹å­å›¾çš„ä¸»é¢˜åœ°å€è¿›å».çœ‹çœ‹å¯¹ä¸å¯¹.


# â¤ï¸â¤ï¸ ä¸‰: è·å–æŸä¸»é¢˜ä¸‹ç¬¬ä¸€å¼ ç…§ç‰‡çš„URL. â¤ï¸â¤ï¸
# ç»“åˆä¸Šé¢çš„ç…§ç‰‡æ•°é‡. å°±èƒ½è·å–åˆ°æŸä¸»é¢˜ä¸‹çš„æ‰€æœ‰ç…§ç‰‡é“¾æ¥äº†.


def get_img_url(url):
    request = urllib.request.Request(url)
    try:
        html = urllib.request.urlopen(request, timeout=20).read()
    except:
        try:
            html = urllib.request.urlopen(request, timeout=20).read()
        except:
            return None
    soup = BeautifulSoup(html, 'lxml')
    try:
        img_url = soup.find(
            'div', {'class':
                    'main-image'}).find('p').find('a').find('img')['src']
    except:
        return None
    return img_url
# bb = get_img_url("http://www.mzitu.com/858")
# print(bb)
# è¿™ä¸¤è¡Œæ˜¯æµ‹è¯• æŸä¸»é¢˜ä¸‹ç¬¬ä¸€å¼ å›¾ç‰‡çš„çœŸå®urlçš„.  äº²æµ‹é€šè¿‡.


# â¤ï¸â¤ï¸ å››: è·å–æŸä¸»é¢˜ä¸‹æ‰€æœ‰ç…§ç‰‡çš„URL. â¤ï¸â¤ï¸


# ç„¶åå°±è¦è·å–æŸä¸»é¢˜ä¸‹æ‰€æœ‰ç…§ç‰‡çš„URLçš„å‡½æ•°
# è¿™æ—¶å€™å°±ç”¨åˆ°äº† ä¸Šé¢ä¸¤ä¸ªå‡½æ•°äº†.
# è¿™ä¸ªå‡½æ•° è¦ä¼ å…¥ä¸€ä¸ªå‚æ•°.ä¹Ÿå°±æ˜¯ä¸»é¢˜çš„URLåœ°å€.
# æ¯ä¸ªä¸»é¢˜éƒ½å¾ªç¯ä¸€é å°±èƒ½è·å–æ‰€æœ‰ä¸»é¢˜çš„æ‰€æœ‰ç…§ç‰‡äº†.
# ä»»åŠ¡ä¹Ÿå°±åªå·®ä¸‹è½½äº†.


def get_img_urls(page1_url):
    page_num = get_page_num(page1_url)
    # è¿™é‡Œå°±ç”¨åˆ°äº† ä¸Šé¢çš„ get_page_num è¿™ä¸ªå‡½æ•°äº†.
    if page_num is None:
        return None
    img_urls = []
    # å®šä¹‰ä¸€ä¸ªæ•°ç»„ æ¥å‚¨å­˜è¯¥ä¸»é¢˜ä¸‹çš„ æ‰€æœ‰ç…§ç‰‡çš„ URL
    for page in range(1, page_num + 1):
        url = page1_url + '/' + str(page)
        # å®é™…ç…§ç‰‡çš„é“¾æ¥åœ°å€ å°±æ˜¯ä¸»é¢˜çš„é“¾æ¥ + / + æ•°é‡
        img_url = get_img_url(url)
        # è¿™é‡Œç”¨åˆ°äº† get_img_url è¿™ä¸ªå‡½æ•°. å¯ä»¥è·å–è¯¥ä¸»é¢˜ä¸‹çš„ç¬¬ä¸€å¼ ç…§ç‰‡.
        # ç°åœ¨æ˜¯åœ¨å¾ªç¯é‡Œé¢. å¾ªç¯æ¬¡æ•°å°±æ˜¯ è¯¥ä¸»é¢˜çš„ç…§ç‰‡æ•°é‡+1
        if img_url is None:
            return None
        else:
            img_urls.append(img_url)
        # æŠŠè·å–åˆ°çš„ url æ·»åŠ åˆ° img_urls è¿™ä¸ªæ•°ç»„é‡Œ.
        # è¿™æ ·å¾ªç¯ä¸‹æ¥ img_urls æ•°ç»„é‡Œé¢å°±æœ‰è¯¥ä¸»é¢˜ä¸‹çš„æ‰€æœ‰ç…§ç‰‡åœ°å€äº†
    return img_urls

# cc = get_img_urls("http://www.mzitu.com/858")
# print(cc)
# è¿™ä¸¤è¡Œæ˜¯æµ‹è¯• æŸä¸»é¢˜ä¸‹æ‰€æœ‰å›¾ç‰‡çš„çœŸå®urlçš„.  äº²æµ‹é€šè¿‡.


# â¤ï¸â¤ï¸ äº”: è·å–æŸä¸»é¢˜åç§°,åˆ›å»ºæœ¬åœ°æ–‡ä»¶å¤¹ç”¨ â¤ï¸â¤ï¸


def get_img_title(page1_url):
    request = urllib.request.Request(page1_url)
    try:
        html = urllib.request.urlopen(request, timeout=20).read()
    except:
        try:
            html = urllib.request.urlopen(request, timeout=20).read()
        except:
            return None
    soup = BeautifulSoup(html, 'lxml')
    # <h2 class="main-title">å¤å…¸æ°”è´¨å‹ç¾å¥³æ–½è¯— é¡¶çº§ç¾è…¿åŠ é…¥èƒ¸åœ†è‡€ç«è¾£èº«ææ€§æ„Ÿåè¶³</h2>
    title = soup.find('h2', {'class': 'main-title'}).get_text()
    # ä¸‹é¢ä¸¤è¡Œæ˜¯å¼‚å¸¸åˆ†æ..
    removeSign = re.compile(r'[\/:*?"<>|]')
    # re å°±æ˜¯æ­£åˆ™è¡¨è¾¾å¼æ¨¡å—
    # re.compile æŠŠæ­£åˆ™è¡¨è¾¾å¼å°è£…èµ·æ¥. å¯ä»¥ç»™åˆ«çš„å‡½æ•°ç”¨. ()é‡Œé¢çš„æ‰æ˜¯çœŸçš„ è¡¨è¾¾å¼.
    # r'[\/:*?"<>|]'  
    # [] è¡¨ç¤ºä¸€ä¸ªå­—ç¬¦é›†;  \å¯¹åé¢çš„è¿›è¡Œè½¬ä¹‰ è‹±æ–‡/æ˜¯ç‰¹æ®Šç¬¦å·; å…¶ä»–çš„æ˜¯æ­£å¸¸ç¬¦å·.  
    title = re.sub(removeSign, '', title)
    # re.sub åœ¨å­—ç¬¦ä¸²ä¸­ æ‰¾åˆ°åŒ¹é…è¡¨è¾¾å¼çš„æ‰€æœ‰å­ä¸². ç”¨å¦ä¸€ä¸ªè¿›è¡Œæ›¿æ¢.è¿™é‡Œç”¨'' å°±æ˜¯åˆ é™¤çš„æ„æ€.
    # å°±æ˜¯è¯´ åˆ é™¤æ ‡é¢˜é‡Œé¢çš„ /:*?"<>| è¿™äº›ç¬¦å·.
    # è‹±æ–‡åˆ›å»ºæ–‡ä»¶å¤¹æ—¶å€™ ä¸èƒ½æœ‰ç‰¹æ®Šç¬¦å·çš„!!!
    return title
# dd = get_img_title("http://www.mzitu.com/858")
# print(dd)
# è¿™ä¸¤è¡Œæ˜¯æµ‹è¯• æŸä¸»é¢˜çš„ä¸»é¢˜å.  äº²æµ‹é€šè¿‡.


# â¤ï¸â¤ï¸ å…­: å®šä¹‰ä¸‹è½½æŸä¸»é¢˜æ‰€æœ‰å›¾ç‰‡çš„å‡½æ•° â¤ï¸â¤ï¸
# ä¸‹è½½è‚¯å®šè¦åˆ›å»ºæ–‡ä»¶å¤¹.è¦ç”¨åˆ°è·¯å¾„.è¿™å°±éœ€è¦ os æ¨¡å—äº†.
# æˆ‘ä»¬æŠŠç…§ç‰‡ å»ºç«‹ä¸ªæ–‡ä»¶å¤¹ ä¸‹è½½åˆ° è„šæœ¬è¿è¡Œçš„ç›®å½•ä¸‹
# os.pathæ¨¡å—ä¸»è¦ç”¨äºæ–‡ä»¶çš„å±æ€§è·å–ï¼Œç»å¸¸ç”¨åˆ°ï¼Œä»¥ä¸‹æ˜¯è¯¥æ¨¡å—çš„å‡ ç§å¸¸ç”¨æ–¹æ³•
# print(os.getcwd())                 # è·å–å¹¶è¾“å‡ºå½“å‰è„šæœ¬æ‰€åœ¨çš„ç›®å½•.
# os.mkdir('./å¦¹å­å›¾')               # åœ¨å½“å‰æ–‡ä»¶å¤¹ä¸‹ å»ºç«‹ å¦¹å­å›¾ æ–‡ä»¶å¤¹.
# os.rmdir('./å¦¹å­å›¾')               # åœ¨å½“å‰æ–‡ä»¶å¤¹ä¸‹ åˆ é™¤ å¦¹å­å›¾ æ–‡ä»¶å¤¹.
# if os.path.exists('./å¦¹å­å›¾'):     # åˆ¤æ–­å½“å‰æ–‡ä»¶å¤¹ æ˜¯å¦å­˜åœ¨   å¦¹å­å›¾è¿™ä¸ªæ–‡ä»¶å¤¹
# if not os.path.exists('./å¦¹å­å›¾'): # åˆ¤æ–­å½“å‰æ–‡ä»¶å¤¹ æ˜¯å¦ä¸å­˜åœ¨ å¦¹å­å›¾è¿™ä¸ªæ–‡ä»¶å¤¹
# æœ¬é¡¹ç›®æˆ‘ä»¬å…ˆåˆ¤æ–­å½“å‰è„šæœ¬æ–‡ä»¶å¤¹ æ˜¯å¦å·²ç»æœ‰å¦¹å­å›¾è¿™ä¸ªæ–‡ä»¶å¤¹å­˜åœ¨.
# å¦‚æœä¸å­˜åœ¨é‚£å°±æ–°å»ºä¸€ä¸ªå¦¹å­å›¾æ–‡ä»¶å¤¹.
# å†åˆ¤æ–­å¦¹å­å›¾æ–‡ä»¶å¤¹ä¸‹ æœ‰æ²¡æœ‰å¯¹åº”çš„å­æ–‡ä»¶å¤¹å­˜åœ¨.


def download_imgs(page1_url):
    img_urls = get_img_urls(page1_url)
    if img_urls is None:
        return None
    if not os.path.exists('./å¦¹å­å›¾'):
        os.mkdir('./å¦¹å­å›¾')
    title = get_img_title(page1_url)
    if title is None:
        return	
    local_path = './å¦¹å­å›¾/' + title
    if not os.path.exists(local_path):
        try:
            os.mkdir(local_path)
        except:
            pass
    if img_urls is None or len(img_urls) == 0:
        return
    else:
        print('--å¼€å§‹ä¸‹è½½' + title + '--')
        for img_url in img_urls:
            img_name = os.path.basename(img_url)
            print('æ­£åœ¨ä¸‹è½½ ' + img_name)
            print('from ' + img_url)
            socket.setdefaulttimeout(10)
            try:
                urllib.request.urlretrieve(img_url, local_path + '/' + img_name)
            except:
                print('ä¸‹è½½' + img_name + 'å¤±è´¥')
        print('--' + title + 'ä¸‹è½½å®Œæˆ--')


# ee = download_imgs("http://www.mzitu.com/858")
# print(ee)
# æˆåŠŸä¸‹è½½ä¸€å¥—ä¸»é¢˜!!!!
# â¤ï¸â¤ï¸ ä¸ƒ: ä¸‹è½½æ‰€æœ‰ä¸»é¢˜çš„å›¾ç‰‡ â¤ï¸â¤ï¸


def craw_meizitu():
    page1_urls = get_page1_urls()
    # è¿™é‡Œç”¨åˆ°äº† ç¬¬ä¸€ä¸ªå‡½æ•°. ä¹Ÿå°±æ˜¯è·å–æ‰€æœ‰ä¸»é¢˜çš„ URL.
    if page1_urls is None or len(page1_urls) == 0:
        return
    else:
        for page1_url in page1_urls:
            # å¾ªç¯ç¬¬å…­æ­¥ æ¥ä¸‹è½½æ‰€æœ‰ä¸»é¢˜çš„URL
            download_imgs(page1_url)


def main():
    craw_meizitu()
if __name__ == '__main__':
    main()
